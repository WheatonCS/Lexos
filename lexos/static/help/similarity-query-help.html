<!-- The Similarity Query Tool -->

<h3 class="help-section-title">The Similarity Query Tool</h3>

<h3 class="help-section-paragraph">
    When you wish to rank the "closeness" between a single document and all other
    documents in your active set, Similarity Query is a good initial probe. The
    rankings are determined by "distance between documents", where small distances
    (near zero) represent documents that are "similar" and "unlike" documents
    have distances closer to one. Similarity Query, as implemented here, is a
    variant of Cosine Similarity.
</h3>

<!-- Options -->
<h3 class="help-section-title">Options</h3>

<ul class="help-section-list" style="list-style-type: none">

    <li><b>Comparison Document</b></li>
    <h3 class="help-section-paragraph">
        Click the "Select" button and then click the radio button for the one
        document to serve as the comparison document. All other active documents
        will be compared to this document. Be sure to click "Generate" to see
        your changes reflected in the data table.
    </h3>

    <li><b>Tokenize</b></li>
    <h3 class="help-section-paragraph">
        By default Lexos tokenizes by tokens, meaning it splits strings of text
        into tokens every time it encounters a space character. For Western languages,
        this means that each token generally corresponds to a word. To tokenize
        by multiple words, you can increase the n-gram size.
    </h3>
    <h3 class="help-section-paragraph">
        For example given the text: "the dog ran" tokenizing by 1-gram tokens
        would produce tokens "the", "dog", "ran". Tokenizing by 2-grams would
        count the instances of bi-grams or pairs of words, thus producing tokens
        "the dog", "dog ran", and so on.
    </h3>
    <h3 class="help-section-paragraph">
        If you wish to tokenize by characters, Lexos will treat every character
        (letters, whitespace, punctuation, etc.) as a separate token. Tokenizing
        by 2-gram characters would produce tokens "th","he","e ", and so on.
        Tokenizing by characters is best used for non-western languages that don't
        have whitespace between tokens such as classical Chinese.
    </h3>

    <li><b>Cull</b></li>
    <h3 class="help-section-paragraph">
        "Culling" is a generic term we use for methods of decreasing the number
        of terms used to generate the DTM based on statistical criteria (as
        opposed to something like applying a stop-word list in Scrubber). Culling
        is optional to use in Lexos. Lexos offers two different methods:
    </h3>
    <h3 class="help-section-paragraph">
        "Use the top ___ words": This method takes a slice of the DTM
        containing only the top N most frequently occurring terms in the
        set of active documents. The default setting is 100, meaning
        Tokenizer will generate the DTM using only the top 100 most
        frequent terms.
    </h3>
    <h3 class="help-section-paragraph">
        "Must be in ___ documents": This method build the DTM using only
        terms that occur in at least N documents. The default setting is
        1. If you have 10 active documents and you want to generate the
        DTM using only terms that appear in all your active documents,
        set the value to 10. <em><b>Note:</b> You can quickly determine
        the number of active documents in your workspace as indicated by
        the counter in the bottom right corner.</em>
    </h3>

</ul>

<!-- Results -->
<h3 class="help-section-title">Results</h3>

<ul class="help-section-list" style="list-style-type: none">

    <li><b>Similarity Query</b></li>
    <h3 class="help-section-paragraph">
        Click the "Generate" button. The results will be shown below in a table,
        that is already sorted by cosine similarity.
    </h3>

    <li><b>Download</b></li>
    <h3 class="help-section-paragraph">
        To download the result click the "Download" button and you will get a
        results <code>.csv</code> file. “CSV” is short for Comma-Separated Values.
        In your downloaded file, a comma will serve as the column delimiter and
        these files can be opened by other programs later, e.g., Excel.
    </h3>

</ul>

<!-- Examples -->
<h3 class="help-section-title">Examples</h3>

<h3 class="help-section-paragraph">
    Visit our public repository on GitHub
    <a target="_blank" href="https://github.com/WheatonCS/Lexos/tree/master/test/test_suite/SimilarityQuery"><u>here</u></a>
    for a test suite of examples you can use to generate Similarity Query results
    before testing your own documents.
</h3>
